# 2024-12-22


### 1 - Solving XOR via simple neural network

XOR (Exclusive OR) outputs 1 if inputs are different, 0 if the same.

Truth table:
(0, 0) → 0
(0, 1) → 1
(1, 0) → 1
(1, 1) → 0

![image](https://github.com/user-attachments/assets/921b8f95-9e43-48db-8751-6c2408fc74e9)

XOR is not linearly separable that means you can't just rotate a line to get it act as a xor function

A neural network doing backpropogation with a hidden layer can solve this since it allows for non-linearty via use of activation functions


```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt

# XOR Dataset
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)
y = np.array([[0], [1], [1], [0]], dtype=np.float32)

# Convert to PyTorch tensors
X_tensor = torch.tensor(X)
y_tensor = torch.tensor(y)

# Define a simple neural network with 1 hidden layer
class XORNet(nn.Module):
    def __init__(self):
        super(XORNet, self).__init__()
        self.hidden = nn.Linear(2, 2)  # 2 input features, 2 neurons in hidden layer
        self.output = nn.Linear(2, 1)  # 2 neurons in hidden, 1 output neuron
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        hidden_output = self.sigmoid(self.hidden(x))
        output = self.sigmoid(self.output(hidden_output))
        return output, hidden_output  # Return both output and hidden layer activations

# Initialize the model, loss function, and optimizer
model = XORNet()
criterion = nn.BCELoss()
optimizer = optim.Adam(model.parameters(), lr=0.1)

# Training the model
epochs = 1000
losses = []
accuracies = []
hidden_activations = []
output_activations = []
weights_hidden = []
biases_hidden = []
weights_output = []
biases_output = []

for epoch in range(epochs):
    # Forward pass
    output, hidden_output = model(X_tensor)
    
    # Compute loss
    loss = criterion(output, y_tensor)
    losses.append(loss.item())
    
    # Compute accuracy
    predicted = (output >= 0.5).float()  # Apply threshold of 0.5
    accuracy = (predicted == y_tensor).float().mean()
    accuracies.append(accuracy.item())
    
    # Backpropagation
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    # Track the parameters
    hidden_activations.append(hidden_output.detach().numpy())
    output_activations.append(output.detach().numpy())
    weights_hidden.append(model.hidden.weight.detach().numpy())
    biases_hidden.append(model.hidden.bias.detach().numpy())
    weights_output.append(model.output.weight.detach().numpy())
    biases_output.append(model.output.bias.detach().numpy())

# Plot the results using subplots
fig, axes = plt.subplots(4, 2, figsize=(12, 10))

# Plot Loss Curve
axes[0, 0].plot(losses)
axes[0, 0].set_title("Loss over Epochs")
axes[0, 0].set_xlabel("Epochs")
axes[0, 0].set_ylabel("Loss")

# Plot Accuracy Curve
axes[0, 1].plot(accuracies)
axes[0, 1].set_title("Accuracy over Epochs")
axes[0, 1].set_xlabel("Epochs")
axes[0, 1].set_ylabel("Accuracy")

# Plot Weight Changes for Hidden Layer
axes[1, 0].plot([np.linalg.norm(w) for w in weights_hidden])
axes[1, 0].set_title("Hidden Layer Weight Norm over Epochs")
axes[1, 0].set_xlabel("Epochs")
axes[1, 0].set_ylabel("Weight Norm")

# Plot Bias Changes for Hidden Layer
axes[1, 1].plot([np.linalg.norm(b) for b in biases_hidden])
axes[1, 1].set_title("Hidden Layer Bias Norm over Epochs")
axes[1, 1].set_xlabel("Epochs")
axes[1, 1].set_ylabel("Bias Norm")

# Plot Weight Changes for Output Layer
axes[2, 0].plot([np.linalg.norm(w) for w in weights_output])
axes[2, 0].set_title("Output Layer Weight Norm over Epochs")
axes[2, 0].set_xlabel("Epochs")
axes[2, 0].set_ylabel("Weight Norm")

# Plot Bias Changes for Output Layer
axes[2, 1].plot([np.linalg.norm(b) for b in biases_output])
axes[2, 1].set_title("Output Layer Bias Norm over Epochs")
axes[2, 1].set_xlabel("Epochs")
axes[2, 1].set_ylabel("Bias Norm")

# Plot Neuron Activations for Hidden Layer (for a single input)
hidden_activations = np.array(hidden_activations)
axes[3, 0].plot(hidden_activations[:, 0], label="Neuron 1")
axes[3, 0].plot(hidden_activations[:, 1], label="Neuron 2")
axes[3, 0].set_title("Hidden Layer Activations over Epochs")
axes[3, 0].set_xlabel("Epochs")
axes[3, 0].set_ylabel("Activation")
axes[3, 0].legend()

# Plot Neuron Activations for Output Layer (for a single input)
output_activations = np.array(output_activations)
axes[3, 1].plot(output_activations[:, 0], label="Output Neuron")
axes[3, 1].set_title("Output Layer Activations over Epochs")
axes[3, 1].set_xlabel("Epochs")
axes[3, 1].set_ylabel("Activation")
axes[3, 1].legend()

plt.tight_layout()
plt.show()

```

![Figure_1](https://github.com/user-attachments/assets/88c84ec4-08f5-49f9-9589-3650375e2578)


